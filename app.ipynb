{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with windowsapi reloader\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkoua\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from flask import Flask, request,jsonify,render_template\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "from PIL import Image\n",
    "import dhash\n",
    "from PIL import ImageFilter\n",
    "import random\n",
    "import tldextract\n",
    "import requests\n",
    "from urllib.parse  import urlparse\n",
    "import hashlib\n",
    "from random import sample \n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = pickle.load(open('model.pkl','rb'))\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "    def func_urlAtSymbol(url):                                    #f2\n",
    "        ats = url.count('@')\n",
    "        if ats >=1 :\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    ####################################### Function that counts the number of dash in the domain name. ########################\n",
    "    def func_urlDasheSymbol(url):\n",
    "        tldextractdomain=tldextract.extract(url).domain\n",
    "        tldextractsubdomain=tldextract.extract(url).subdomain    #f3\n",
    "        Dashe = tldextractdomain.count('-') \n",
    "        Dashe1 = tldextractsubdomain.count('-')\n",
    "        if (Dashe>1 ) or (Dashe1 >=4):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    ######################################  Function that checks if the domain name is an IPV4 address ##############################\n",
    "    def func_ipAddress(url) :\n",
    "        tldextractsubdomain=tldextract.extract(url).subdomain   #f8\n",
    "        tldextractdomain=tldextract.extract(url).domain\n",
    "        #print(tldextractsubdomain)\n",
    "        #print(tldextractdomain)\n",
    "        fffff = (re.findall(r'[0-9]+(?:\\.[0-9]+){3}', tldextract.extract(url).subdomain ))\n",
    "        ffff = (re.findall(r'[0-9]+(?:\\.[0-9]+){3}', tldextract.extract(url).domain))\n",
    "        #print(fffff)\n",
    "        #print(ffff)\n",
    "        if (fffff or ffff):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    ##################################### Function that checks the length of an url ##################################################\n",
    "    def func_urLength(url):                                      #f9\n",
    "        #for j in range(len(data)):\n",
    "            urlength = len(url) \n",
    "            if(urlength > 80):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1 \n",
    "    ############################### Function that checks the number of dots the resource ################################\n",
    "    def func_urlDotSymbol(url):                                  #f4\n",
    "            dots= urlparse(url).netloc\n",
    "            if dots.count('.')<= 3 :\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "    ############################## ############Decision groups########################################################################\n",
    "    def Goodfunc_urlAtSymbol(Feature2,Feature21,Feature22):\n",
    "        if (Feature2==-1 or Feature21==-1 or Feature22 ==-1):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def Goodfunc_urlDasheSymbol(Feature3,Feature31,Feature32):\n",
    "        if (Feature3==-1 or Feature31==-1 or Feature32 ==-1):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def Goodfunc_urlDotSymbol(Feature4,Feature41,Feature42):\n",
    "        if (Feature4==-1 or Feature41==-1 or Feature42==-1):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def Goodfunc_CheckpasswordCreditcard(Feature5,Feature51,Feature52):\n",
    "        if (Feature5==-1 or Feature51==-1 or Feature52 ==-1):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    def Goodfunc_MatchDomainTitle(Feature6,Feature61,Feature62):\n",
    "        if (Feature6==-1 or Feature61==-1 or Feature62==-1):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    def Good_func_NRP(Feature7,Feature71,Feature72):\n",
    "        if (Feature7 == -1) or (Feature71==-1 ) or (Feature72==-1):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    def Good_func_ipAddress(Feature8,Feature81,Feature82):\n",
    "        if (Feature8 ==-1) or (Feature81==-1 ) or (Feature82==-1):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    def  Good_func_urLength(Feature9,Feature91,Feature92):\n",
    "        if ((Feature9==-1 or Feature91 ==-1 or Feature92) ==-1):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def hamming_distance(string1, string2): \n",
    "        distance = 0\n",
    "        L = len(string1)\n",
    "        for i in range(L):\n",
    "            if string1[i] != string2[i]:\n",
    "                distance += 1\n",
    "        return distance\n",
    "    def concat_ints(a, b):\n",
    "        return a*(10**len(str(b)))+b\n",
    "    ############################################ First check on the URL\n",
    "    def checonnection(data):\n",
    "        for j in range(len(data)):\n",
    "            urlj = data[j]\n",
    "            try :\n",
    "                request = requests.get(urlj)\n",
    "                if request.status_code == 200:\n",
    "                     data1.append(urlj)\n",
    "            except:\n",
    "                print(\"Fail connection\")\n",
    "        return data1############################################ First check on the URL\n",
    "    def checonnection(data):\n",
    "        for j in range(len(data)):\n",
    "            urlj = data[j]\n",
    "            try :\n",
    "                request = requests.get(urlj)\n",
    "                if request.status_code == 200:\n",
    "                     data1.append(urlj)\n",
    "            except:\n",
    "                print(\"Fail connection\")\n",
    "        return data1\n",
    "    '''\n",
    "    For rendering results on HTML GUI\n",
    "    '''\n",
    "    url = request.form(\"url\")\n",
    "    data1 = [('{}'.format(url))]\n",
    "    \n",
    "        \n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    #try:\n",
    "    for n in range(len(data1)):\n",
    "                    url = data1[n]\n",
    "                    links =[]\n",
    "                    RP1=[]\n",
    "                    link2 =[]\n",
    "                    RP3=[]\n",
    "                    links3 =[]\n",
    "                    RP3=[]\n",
    "                    try :\n",
    "                        driver = webdriver.Chrome('C:/Users/bkoua/Eprojet_/APIPHISHING/chromedriver.exe')\n",
    "                        driver.implicitly_wait(60) \n",
    "                        resp= driver.get(url)\n",
    "                        driver.get_screenshot_as_file('C:/Users/bkoua/Eprojet_/APIPHISHING/screenshot1.png')\n",
    "                        driver.close()\n",
    "                        response = requests.get(url)\n",
    "                        html = response.text         \n",
    "                        soup = bs(html, 'lxml')\n",
    "                        links = ([a.get('href') for a in soup.findAll('a', attrs={'href': re.compile(\"^https://\")} )] or\n",
    "                        [a.get('href') for a in soup.findAll('a', attrs={'href': re.compile(\"^http://\")} )] or\n",
    "                        [a.get('href') for a in soup.findAll('a', attrs={'href': re.compile(\"^#\")} )] or \n",
    "                        [a.get('href') for a in soup.findAll('a', attrs={'href': re.compile(\"^/\")} )] or \n",
    "                        [a.get('href') for a in soup.findAll('a', attrs={'href': re.compile(\"^[A-Za-z0-9]\")} )] or \n",
    "                        [a.get('href') for a in soup.findAll('a', attrs={'href': re.compile(\"^https://[A-Za-z0-9]\")} )] or\n",
    "                        [a.get('href') for a in soup.findAll('a', attrs={'href': re.compile(\"^http://[A-Za-z0-9]\")} )]   or\n",
    "                        [link.get('href') for link in soup.findAll('link', attrs={'href': re.compile(\"^http://[A-Za-z0-9]\")} )] or \n",
    "                        [script.get('src') for script in soup.findAll('script', attrs={'src': re.compile(\"^http://[A-Za-z0-9]\")})] or\n",
    "                        [link.get('href') for link in soup.findAll('link', attrs={'href': re.compile(\"^https://[A-Za-z0-9]\")} )] or \n",
    "                        [script.get('src') for script in soup.findAll('script', attrs={'src': re.compile(\"^https://[A-Za-z0-9]\")} )]or\n",
    "                        [link.get('href') for link in soup.findAll('link', attrs={'href': re.compile(\"^#\")} )] or \n",
    "                        [script.get('src') for script in soup.findAll('script', attrs={'src': re.compile(\"^#\")})]or\n",
    "                        [link.get('href') for link in soup.findAll('link', attrs={'href': re.compile(\"^[A-Za-z0-9]\")} )] or \n",
    "                        [script.get('src') for script in soup.findAll('script', attrs={'src': re.compile(\"^[A-Za-z0-9]\")} )])\n",
    "                        #print(links)\n",
    "                        RP1 =([a.get('href') for a in soup.findAll('a', attrs={'href': re.compile(\"^#\")} )] or\n",
    "                         [link.get('href') for link in soup.findAll('link', attrs={'href': re.compile(\"^#\")} )]or\n",
    "                         [script.get('src') for script in soup.findAll('script', attrs={'src': re.compile(\"^#\")})])\n",
    "\n",
    "                        hash_object = hashlib.sha3_224(soup.encode()).hexdigest()\n",
    "                        #print(hash_object.hexdigest())\n",
    "                        print(url)\n",
    "                    except:\n",
    "                        pass\n",
    "                    def func_NRP(RP1):\n",
    "                        if len(RP1)>=1:\n",
    "                            return -1\n",
    "                        else:\n",
    "                            return 1\n",
    "\n",
    "                    newL =[]\n",
    "                    def selct_url(links,url):\n",
    "                        list11=[]\n",
    "                        if(len(links)>=2):\n",
    "                            list_of_random_items=random.sample(links,2)\n",
    "                            #print(list_of_random_items)\n",
    "                            newLink=list_of_random_items[0]\n",
    "                            #print(list_of_random_items[0])\n",
    "                            newLinktext1=list_of_random_items[1]\n",
    "                            #print(list_of_random_items[1])\n",
    "                            list11.append(newLink) \n",
    "                            list11.append(newLinktext1)\n",
    "                            #print(list11)\n",
    "                            #return(newLink,newLinktext1)\n",
    "                        elif(len(links) ==1):\n",
    "                            list_of_random_items=random.sample(links,1)\n",
    "                            #print(list_of_random_items)\n",
    "                            newLink=list_of_random_items[0]\n",
    "                            #print(newLink)\n",
    "                            newLinktext1= url\n",
    "                            list11.append(newLink) \n",
    "                            list11.append(newLinktext1)\n",
    "                            #return(newLink,newLinktext1)\n",
    "                            #print(list11)\n",
    "                        else:\n",
    "                            newLink= url\n",
    "                            newLinktext1 = url\n",
    "                            list11.append(newLink) \n",
    "                            list11.append(newLinktext1)\n",
    "                            #print(list11)\n",
    "                        return list11\n",
    "                    newL= selct_url(links,url)   \n",
    "                    newLink = newL[0] \n",
    "                    newLinktext1=newL[1]\n",
    "                    #print(newLinktext1)\n",
    "                    #print(newLinktext2)\n",
    "                    ######################## #### ############################################################################\n",
    "                    Feature2 = func_urlAtSymbol(url)\n",
    "                    Feature3 = func_urlDasheSymbol(url)\n",
    "                    Feature4 = func_urlDotSymbol(url)  \n",
    "                    Feature8 = func_ipAddress(url)\n",
    "                    Feature9 = func_urLength(url)\n",
    "\n",
    "                    ############################## Test on the content of the page ########################################################\n",
    "                    ###################################### Function that verifies if the page asks for the password ###################\n",
    "                    def func_CheckpasswordCreditcard(url):            #f6\n",
    "                        response = requests.get(url)\n",
    "                        html = response.text\n",
    "                        soup = bs(html, 'lxml')\n",
    "                        pwdCredit =( [input.get('type') for input in soup.findAll('input', attrs={'type': re.compile(\"^idcard\")} )] or \n",
    "                        [input.get('type') for input in soup.findAll('input', attrs={'type': re.compile(\"^password\")} )]or\n",
    "                        [label.get('for') for label in soup.findAll('label', attrs={'for': re.compile(\"^j_pin\")} )]or\n",
    "                        [label.get('for') for label in soup.findAll('label', attrs={'for': re.compile(\"^j_username\")} )] or\n",
    "                        [label.get('for') for label in soup.findAll('label', attrs={'for': re.compile(\"^j_user_no\")} )])\n",
    "                        if (len(pwdCredit) == 0):\n",
    "                            return 1\n",
    "                        else:\n",
    "                            return -1\n",
    "                    def func_MatchDomainTitle(url): #f5\n",
    "                        import tldextract\n",
    "                        import requests\n",
    "                        from bs4 import BeautifulSoup as bs\n",
    "                        subdm = tldextract.extract(url).subdomain\n",
    "                        #print(subdm)\n",
    "                        dmr = tldextract.extract(url).domain\n",
    "                        #print(dmr)\n",
    "                        consubdm =''.join(e for e in subdm.lower() if e.isalnum())\n",
    "                        consdmr = ''.join(e for e in dmr.lower() if e.isalnum())\n",
    "                        #print(consubdm)\n",
    "                        #print(consdmr)\n",
    "                        response = requests.get(url)\n",
    "                        html = response.text\n",
    "                        soup = bs(html, 'lxml')\n",
    "                        try :\n",
    "                            if (soup.title.string):\n",
    "                                    #lisdmr = re.search(consdmr , ''.join(e for e in soup.title.string.lower() if e.isalnum()))\n",
    "                                if (re.search(consdmr , ''.join(e for e in soup.title.string.lower() if e.isalnum()))==None):\n",
    "                                    result1 = -1\n",
    "                                else:\n",
    "                                    result1 = 1\n",
    "                                result= result1\n",
    "                            else:\n",
    "                                result = -1\n",
    "                        except:\n",
    "                              result = -1\n",
    "                        return result\n",
    "                    Feature5 = func_CheckpasswordCreditcard(url)\n",
    "                    Feature6 = func_MatchDomainTitle(url)\n",
    "                    Feature7 = func_NRP(RP1)\n",
    "                    #print(url)\n",
    "                    ##$$print(newLink)\n",
    "                    ##$$print(newLinktext1)\n",
    "                    ############################### Closing the first page #########################################################\n",
    "                    links2 =[]\n",
    "                    RP2=[]\n",
    "                    def checonnectionurll2(urll):\n",
    "                        try :\n",
    "                            request = requests.get(urll)\n",
    "                            if request.status_code == 200:\n",
    "                                urll1 = urll\n",
    "                        except:\n",
    "                            print(\"Fail connection\")\n",
    "                            urll1= url\n",
    "\n",
    "                        return urll1\n",
    "\n",
    "                    try:    \n",
    "                        url2 = checonnectionurll2(newLink)\n",
    "                        driver2 = webdriver.Chrome('C:/Users/bkoua/Eprojet_/APIPHISHING/chromedriver.exe')\n",
    "                        driver2.implicitly_wait(60) \n",
    "                        resp2= driver2.get(url2)\n",
    "                        driver2.get_screenshot_as_file('C:/Users/bkoua/Eprojet_/APIPHISHING/screenshot2.png')\n",
    "                        driver2.close()\n",
    "                        response2 = requests.get(url2)\n",
    "                        html2 = response2.text\n",
    "                        soup2 = bs(html2, 'lxml')                                                     \n",
    "                        #links = soup.find_all('href')\n",
    "                        links2 = ([a.get('href') for a in soup2.findAll('a', attrs={'href': re.compile(\"^https://\")} )] or \n",
    "                        [a.get('href') for a in soup2.findAll('a', attrs={'href': re.compile(\"^http://\")} )]  or \n",
    "                        [a.get('href') for a in soup2.findAll('a', attrs={'href': re.compile(\"^#\")} )] \n",
    "                        or [a.get('href') for a in soup2.findAll('a', attrs={'href': re.compile(\"^/\")} )] or \n",
    "                        [a.get('href') for a in soup2.findAll('a', attrs={'href': re.compile(\"^[A-Za-z0-9]\")} )] or \n",
    "                        [a.get('href') for a in soup2.findAll('a', attrs={'href': re.compile(\"^https://[A-Za-z0-9]\")} )] or\n",
    "                        [a.get('href') for a in soup2.findAll('a', attrs={'href': re.compile(\"^http://[A-Za-z0-9]\")} )] or\n",
    "                        [link.get('href') for link in soup2.findAll('link', attrs={'href': re.compile(\"^http://[A-Za-z0-9]\")} )] or \n",
    "                        [script.get('src') for script in soup2.findAll('script', attrs={'src': re.compile(\"^http://[A-Za-z0-9]\")})] or\n",
    "                        [link.get('href') for link in soup2.findAll('link', attrs={'href': re.compile(\"^https://[A-Za-z0-9]\")} )] or \n",
    "                        [script.get('src') for script in soup2.findAll('script', attrs={'src': re.compile(\"^https://[A-Za-z0-9]\")} )]or\n",
    "                        [link.get('href') for link in soup2.findAll('link', attrs={'href': re.compile(\"^#\")} )] or \n",
    "                        [script.get('src') for script in soup2.findAll('script', attrs={'src': re.compile(\"^#\")})]or\n",
    "                        [link.get('href') for link in soup2.findAll('link', attrs={'href': re.compile(\"^[A-Za-z0-9]\")} )] or \n",
    "                        [script.get('src') for script in soup2.findAll('script', attrs={'src': re.compile(\"^[A-Za-z0-9]\")} )])\n",
    "                        ##$$links2\n",
    "                        RP2 =([a.get('href') for a in soup2.findAll('a', attrs={'href': re.compile(\"^#\")} )] or\n",
    "                         [link.get('href') for link in soup2.findAll('link', attrs={'href': re.compile(\"^#\")} )]or\n",
    "                         [script.get('src') for script in soup2.findAll('script', attrs={'src': re.compile(\"^#\")})])\n",
    "                        hash_object2 = hashlib.sha3_224(soup2.encode())\n",
    "                        #print(hash_object2.hexdigest())\n",
    "                    except:\n",
    "                        pass\n",
    "                    def func_NRP2(RP2):\n",
    "                        if len(RP2)>=1:\n",
    "                            return -1\n",
    "                        else:\n",
    "                            return 1\n",
    "\n",
    "\n",
    "                    def selct_url2(links2, newLink):\n",
    "                        if(len(links2)>=2):\n",
    "                            list_of_random_items2=random.sample(links2,2)\n",
    "                            newLink2=list_of_random_items2[0]\n",
    "                            #newLinktext1=list_of_random_items[1]\n",
    "                        #return(newLink,newLinktext1)\n",
    "                        elif(len(links2) ==1):\n",
    "                            list_of_random_items2=random.sample(links2,1)\n",
    "                            newLink2=list_of_random_items2[0]\n",
    "                            #newLinktext1= url\n",
    "                        #return(newLink,newLinktext1)\n",
    "                        else:\n",
    "                            newLink2= newLink\n",
    "                            #newLinktext1 = url\n",
    "                        return newLink2\n",
    "                    newLink2 = selct_url2(links2, newLink)\n",
    "                    ############################ Test on url ############################################\n",
    "                    Feature21 = func_urlAtSymbol(url2)\n",
    "                    Feature31 = func_urlDasheSymbol(url2)\n",
    "                    Feature41= func_urlDotSymbol(url2)  \n",
    "                    Feature81 = func_ipAddress(url2)\n",
    "                    Feature91 = func_urLength(url2)\n",
    "\n",
    "                    ############################## Test on the content of the page ######################################################\n",
    "                    def func_CheckpasswordCreditcard2(url2):            #f6\n",
    "                        response2 = requests.get(url2)\n",
    "                        html2 = response2.text\n",
    "                        soup2 = bs(html2, 'lxml')\n",
    "                        pwdCredit2 =( [input.get('type') for input in soup2.findAll('input', attrs={'type': re.compile(\"^idcard\")} )] or\n",
    "                        [input.get('type') for input in soup2.findAll('input', attrs={'type': re.compile(\"^password\")} )] or \n",
    "                        [label.get('for') for label in soup2.findAll('label', attrs={'for': re.compile(\"^j_pin\")} )]or\n",
    "                        [label.get('for') for label in soup2.findAll('label', attrs={'for': re.compile(\"^j_username\")} )] or\n",
    "                        [label.get('for') for label in soup2.findAll('label', attrs={'for': re.compile(\"^j_user_no\")} )])\n",
    "                        if (len(pwdCredit2) == 0):\n",
    "                            return 1\n",
    "                        else:\n",
    "                            return -1\n",
    "                    def func_MatchDomainTitle2(url2):                  #f5\n",
    "\n",
    "                        subdm2 = tldextract.extract(url2).subdomain\n",
    "                        dmr2 = tldextract.extract(url2).domain\n",
    "                        consubdm2 =''.join(e for e in subdm2.lower() if e.isalnum())\n",
    "                        consdmr2 = ''.join(e for e in dmr2.lower() if e.isalnum())\n",
    "                        response2 = requests.get(url2)\n",
    "                        html2 = response2.text\n",
    "                        soup2 = bs(html2, 'lxml')\n",
    "                        try :\n",
    "                            if ((soup2.title.string) and (soup2.title.string.lower())):\n",
    "                                    #lisdmr = re.search(consdmr , ''.join(e for e in soup.title.string.lower() if e.isalnum()))\n",
    "                                if (re.search(consdmr2 , ''.join(e for e in soup2.title.string.lower() if e.isalnum()))==None):\n",
    "                                    result1 = -1\n",
    "                                else:\n",
    "                                    result1 = 1\n",
    "                                result= result1\n",
    "                            else:\n",
    "                                result = -1\n",
    "                        except:\n",
    "                            result = -1\n",
    "                        return result\n",
    "                                #print(lisdmr\n",
    "                    ##$$print(newLink2)\n",
    "                    #driver2.close()\n",
    "\n",
    "                    Feature51 = func_CheckpasswordCreditcard2(url2)\n",
    "                    Feature61 = func_MatchDomainTitle2(url2)\n",
    "                    Feature71 = func_NRP(RP2)\n",
    "                    ################################# Closing of the second web page#######################################################\n",
    "\n",
    "                    def checonnectionurll3(urll):\n",
    "                        try :\n",
    "                            request = requests.get(urll)\n",
    "                            if request.status_code == 200:\n",
    "                                urll1 = urll\n",
    "                        except:\n",
    "                            print(\"Fail connection\")\n",
    "                            urll1 = url2\n",
    "\n",
    "                        return urll1\n",
    "                    try:\n",
    "                        url3 = checonnectionurll3(newLink2)\n",
    "                        driver3 = webdriver.Chrome('C:/Users/bkoua/Eprojet_/APIPHISHING/chromedriver.exe')\n",
    "                        driver3.implicitly_wait(20) \n",
    "                        resp3= driver3.get(url3)\n",
    "                        driver3.get_screenshot_as_file('C:/Users/bkoua/Eprojet_/APIPHISHING/screenshot3.png')\n",
    "                        driver3.close()\n",
    "                        response3 = requests.get(url3)\n",
    "                        html3 = response3.text\n",
    "                        soup3 = bs(html3, 'lxml')\n",
    "                        RP3 =([a.get('href') for a in soup3.findAll('a', attrs={'href': re.compile(\"^#\")} )] or\n",
    "                         [link.get('href') for link in soup3.findAll('link', attrs={'href': re.compile(\"^#\")} )]or\n",
    "                         [script.get('src') for script in soup3.findAll('script', attrs={'src': re.compile(\"^#\")})])\n",
    "                    except:\n",
    "                        pass\n",
    "                    #hash_object3 = hashlib.md5(soup3.encode())\n",
    "                    #print(hash_object3.hexdigest())\n",
    "                    def func_NRP3(RP3):\n",
    "                        if len(RP3)>=1:\n",
    "                            return -1\n",
    "                        else:\n",
    "                            return 1\n",
    "\n",
    "                    Feature22 = func_urlAtSymbol(url3)\n",
    "                    Feature32 = func_urlDasheSymbol(url3)\n",
    "                    Feature42 = func_urlDotSymbol(url3) \n",
    "                    Feature82 = func_ipAddress(url3)\n",
    "                    Feature92 = func_urLength(url3)\n",
    "\n",
    "                    ############################## Test on the page content  #######################################################\n",
    "                    def func_CheckpasswordCreditcard3(url3):            #f6\n",
    "                        response3 = requests.get(url3)\n",
    "                        html3 = response3.text\n",
    "                        soup3 = bs(html3, 'lxml')\n",
    "                        pwdCredit3 = ([input.get('type') for input in soup3.findAll('input', attrs={'type': re.compile(\"^idcard\")} )] or\n",
    "                        [input.get('type') for input in soup3.findAll('input', attrs={'type': re.compile(\"^password\")} )]or\n",
    "                                             [label.get('for') for label in soup3.findAll('label', attrs={'for': re.compile(\"^j_pin\")} )]or\n",
    "                        [label.get('for') for label in soup3.findAll('label', attrs={'for': re.compile(\"^j_username\")} )] or\n",
    "                        [label.get('for') for label in soup3.findAll('label', attrs={'for': re.compile(\"^j_user_no\")} )])\n",
    "                        if (len(pwdCredit3) == 0):\n",
    "                            return 1\n",
    "                        else:\n",
    "                            return -1\n",
    "                    def func_MatchDomainTitle3(url3): #f5\n",
    "\n",
    "                        subdm3 = tldextract.extract(url3).subdomain\n",
    "                        dmr3 = tldextract.extract(url3).domain\n",
    "                        consubdm3 =''.join(e for e in subdm3.lower() if e.isalnum())\n",
    "                        consdmr3 = ''.join(e for e in dmr3.lower() if e.isalnum())\n",
    "                        response3 = requests.get(url3)\n",
    "                        html3 = response3.text\n",
    "                        soup3 = bs(html3, 'lxml')\n",
    "                        try :\n",
    "                            if ((soup3.title.string) and (soup3.title.string.lower())):\n",
    "                                    #lisdmr = re.search(consdmr , ''.join(e for e in soup.title.string.lower() if e.isalnum()))\n",
    "                                if (re.search(consdmr3 , ''.join(e for e in soup3.title.string.lower() if e.isalnum()))==None):\n",
    "                                    result1 = -1\n",
    "                                else:\n",
    "                                    result1 = 1\n",
    "                                result= result1\n",
    "                            else:\n",
    "                                result = -1\n",
    "                        except:\n",
    "                            result = -1\n",
    "                        return result\n",
    "                            #print(lisdmr)\n",
    "                        #print(lisdmr2)\n",
    "                    #print(newLink3)\n",
    "                    #driver3.close()\n",
    "                    Feature52 = func_CheckpasswordCreditcard3(url3)\n",
    "                    Feature62 = func_MatchDomainTitle3(url3)\n",
    "                    Feature72 = func_NRP3(RP3)\n",
    "                    ########################################## Closing the last web page##############################################\n",
    "                    ################################################# hash of the second link select on url###############################################\n",
    "                    def checonnectionurll4(urll):\n",
    "                        try :\n",
    "                            request = requests.get(urll)\n",
    "                            if request.status_code == 200:\n",
    "                                urll1 = urll\n",
    "                        except:\n",
    "                            print(\"Fail connection\")\n",
    "                            urll1 = url\n",
    "\n",
    "                        return urll1\n",
    "                                #return url2\n",
    "                    try:\n",
    "                        newLinktext1 = checonnectionurll4((newLinktext1))\n",
    "                        response4 = requests.get(newLinktext1)\n",
    "                        html4 = response4.text\n",
    "                        soup4 = bs(html4, 'lxml')\n",
    "                        hash_object4 = hashlib.sha3_224(soup4.encode())\n",
    "                       # print(hash_object4.hexdigest())\n",
    "                    except:\n",
    "                        pass\n",
    "                    GFeature2 = Goodfunc_urlAtSymbol(Feature2,Feature21,Feature22)\n",
    "                    GFeature3 = Goodfunc_urlDasheSymbol(Feature3,Feature31,Feature32)\n",
    "                    GFeature4 = Goodfunc_urlDotSymbol(Feature4,Feature41,Feature42)\n",
    "                    GFeature5 = Goodfunc_CheckpasswordCreditcard(Feature5,Feature51,Feature52)\n",
    "                    GFeature6 = Goodfunc_MatchDomainTitle(Feature6,Feature61,Feature62)\n",
    "                    #GFeature7 = Good_func_NRP(Feature7,Feature71,Feature72)\n",
    "                    GFeature8 = Good_func_ipAddress(Feature8,Feature81,Feature82)\n",
    "                    GFeature9 = Good_func_urLength(Feature9,Feature91,Feature92)\n",
    "\n",
    "\n",
    "                    with Image.open(\"C:/Users/bkoua/Eprojet_/APIPHISHING/screenshot1.png\") as H1 :\n",
    "                        row, col = dhash.dhash_row_col(H1 )\n",
    "                        #hash1.show() \n",
    "                        hash1= dhash.format_hex(row, col)\n",
    "                   # print(hash1)\n",
    "                    with Image.open(\"C:/Users/bkoua/Eprojet_/APIPHISHING/screenshot2.png\") as H2 :\n",
    "                        row2, col2 = dhash.dhash_row_col(H2 )\n",
    "                        #hash1.show() \n",
    "                        hash2= dhash.format_hex(row2, col2)\n",
    "                  #   print(hash2)\n",
    "                    with Image.open(\"C:/Users/bkoua/Eprojet_/APIPHISHING/screenshot3.png\") as H3 :\n",
    "                        row3, col3 = dhash.dhash_row_col(H3 )\n",
    "                        #hash1.show() \n",
    "                        hash3= dhash.format_hex(row3, col3)\n",
    "                  #  print(hash3)\n",
    "                    import csv\n",
    "\n",
    "                    #################################### Text on similarity #############################################################\n",
    "                    Htesttext2 = hamming_distance(hashlib.sha3_224(soup4.encode()).hexdigest(),hashlib.sha3_224(bs(requests.get(url).text, 'lxml').encode()).hexdigest())\n",
    "                    Hreftext1 = hamming_distance(hashlib.sha3_224( bs(requests.get(url2).text, 'lxml').encode()).hexdigest(),hashlib.sha3_224(soup4.encode()).hexdigest())\n",
    "\n",
    "                    testscore1 = (Htesttext2 - Hreftext1)\n",
    "                    #def Textscore(testscore1):\n",
    "                        #if(testscore1==0 or testscore1>5 or testscore1<-5):\n",
    "                           # return -1\n",
    "                        #else:\n",
    "                            #return 1\n",
    "\n",
    "                   #################################### perceptual similarity############################################################\n",
    "                    Href = hamming_distance(hash2,hash3)\n",
    "                    Htest = hamming_distance(hash3,hash1)\n",
    "                    score1 = (Htest - Href)\n",
    "                    #def score(score1):\n",
    "                        #if (score1 >10 or score1 <-10 or score1==0):\n",
    "                           # return -1\n",
    "                       # else:\n",
    "                           # return 1\n",
    "\n",
    "\n",
    "                    with open('C:/Users/bkoua/Eprojet_/APIPHISHING/LIONEL.csv', mode='w') as score_file:\n",
    "                        fieldnames = ['URL','Perceptual_similarity','Text_similarity','TesturlAtSymbol','TesturlDasheSymbol','TesturlDotSymbol',\n",
    "                                      'TestGoodCheckpwdCreditcard','TestGoodMatchDomainTitle','TestIPAdress','TestGoodurLength']\n",
    "                        writer = csv.DictWriter(score_file, fieldnames=fieldnames)\n",
    "\n",
    "                        writer.writerow({ 'URL': url, 'Perceptual_similarity': score1,'Text_similarity': testscore1,'TesturlAtSymbol':GFeature2, 'TesturlDasheSymbol':GFeature3,\n",
    "                                         'TesturlDotSymbol':GFeature4,'TestGoodCheckpwdCreditcard': GFeature5,'TestGoodMatchDomainTitle':GFeature6,\n",
    "                                         'TestIPAdress':GFeature8,'TestGoodurLength':Feature9})\n",
    "                            #score_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                            #file=open('/home/leonel/projet_/score.csv', 'w') 'TestGoodRP':GFeature7, 'TestGoodRP',\n",
    "                            #file.write(\"d3\\n\")\n",
    "                    score_file.close()\n",
    "                    #print(testscore1)\n",
    "                    #print(Textscore(testscore1))\n",
    "                    #print(score1 )\n",
    "                    #print(score(score1))\n",
    "                    print(n)\n",
    "                    #print(soup)\n",
    "                    #print(soup2)\n",
    "                    #print(soup4)\n",
    "\n",
    "                    #print(hash_object.hexdigest())\n",
    "                    #print(hash_object2.hexdigest())\n",
    "                    #print(hash_object4.hexdigest())\n",
    "                    #print(Textscore(testscore1))\n",
    "                    #print(hash1)\n",
    "                    #print(hash2)\n",
    "                    #print(hash3)\n",
    "                    #print(score(score1))    \n",
    "                   # except:\n",
    "                    #    error_text = \"May be the request has problem. Please try to verify your Url and try again \"\n",
    "\n",
    "\n",
    "    def firstElmtFirstLine(file):\n",
    "        model = pickle.load(open('model.pkl','rb'))\n",
    "        with open('C:/Users/bkoua/Eprojet_/APIPHISHING/LIONEL.csv', 'r') as f:\n",
    "            ligne = f.readline()\n",
    "            premierElement = ligne.split(',')[0:10]\n",
    "            converted_list = []\n",
    "            for element in premierElement:\n",
    "                converted_list.append(element.strip())\n",
    "                final_features = [converted_list]\n",
    "                prediction = model.predict(final_features)\n",
    "                if (prediction == 1):\n",
    "                    text = 'secure '\n",
    "                else:\n",
    "                    text = 'is not secure '\n",
    "        return text\n",
    "    output = firstElmtFirstLine('C:/Users/bkoua/Eprojet_/APIPHISHING/LIONEL.csv')\n",
    "    return render_template('index.html', prediction_text=' This website {}'.format(output))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def firstElmtFirstLine(file):\n",
    "        #model = pickle.load(open('model.pkl','rb'))\n",
    "        with open('C:/Users/bkoua/Eprojet_/APIPHISHING/LIONEL.csv', 'r') as f:\n",
    "            ligne = f.readline()\n",
    "            premierElement = ligne.split(',')[0:10]\n",
    "            converted_list = []\n",
    "            for element in premierElement:\n",
    "                converted_list.append(element.strip())\n",
    "                final_features = [converted_list]\n",
    "            model = pickle.load(open('model.pkl','rb'))\n",
    "            prediction = model.predict(final_features)\n",
    "            if (prediction[0] == 1):\n",
    "                text = 'secure' \n",
    "            else:\n",
    "                text = 'is not secure'            \n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'secure'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "output = firstElmtFirstLine('C:/Users/bkoua/Eprojet_/APIPHISHING/LIONEL.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secure \n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open('model.pkl','rb'))\n",
    "final_features=[['0','0','1','1','1','1','-1','1','1']]\n",
    "prediction = model.predict(final_features)\n",
    "if (prediction[0] == 1):\n",
    "    text = 'secure '\n",
    "else:\n",
    "    text = 'is not secure '\n",
    "print(text)\n",
    "#print (prediction.value())#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('C:/Users/bkoua/Eprojet_/APIPHISHING/LIONEL.csv', 'r') as f:\n",
    "    ligne = f.readline()\n",
    "    premierElement = ligne.split(',')[0:10]\n",
    "    converted_list = []\n",
    "    for element in premierElement:\n",
    "        converted_list.append(element.strip())\n",
    "        final_features = [converted_list]\n",
    "        \n",
    "model = pickle.load(open('model.pkl','rb'))\n",
    "prediction = model.predict(final_features)\n",
    "premierElement\n",
    "converted_list\n",
    "final_features\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codescore1(score1): \n",
    "    if( score1 == 0 or score1>10 or score1<-10):\n",
    "        score1 = -1\n",
    "    else:\n",
    "        score1 = 1\n",
    "    return score1\n",
    "def codetestscore1(testscore1):\n",
    "    if( testscore1 == 0 or testscore1>5 or testscore1<-5):\n",
    "        testscore1 = -1\n",
    "    else:\n",
    "        testscore1 = 1\n",
    "    return testscore1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = 1\n",
    "codetestscore1(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
